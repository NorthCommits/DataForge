{"documents": [{"url": "https://nexla.com/ai-infrastructure/llm-hallucination/", "title": "LLM Hallucination—Types, Causes, and Solutions - Nexla", "content": "There is a critical need to tackle the issue of hallucination in LLMs and ensure responsible application of Generative AI and LLM technology. | LLM hallucination causes | * Training data issues * Model limitations * Token size constraints * Nuanced language understanding difficulties. | Best practices to avoid hallucination | * Pre-processing and input control * Adjusting model parameters * Implementing moderation layers * Monitoring and continuous improvement * Enhancing context and data. | Improving training data to reduce hallucination | Nexla can significantly enhance the quality and reliability of data for training and updating LLM models through features like  * Data quality and consistency checks * Real-time data integration * Customized data streams for domain adaptation * Feedback loops for continuous learning and improvement.", "published_at": null, "source_score": 0.98502}, {"url": "https://www.ibm.com/think/topics/ai-hallucinations", "title": "What Are AI Hallucinations? - IBM", "content": "# What are AI hallucinations? ## What are AI hallucinations? If an AI model is trained on a dataset comprising biased or unrepresentative data, it may hallucinate patterns or features that reflect these biases. In order to prevent hallucinations, ensure that AI models are trained on diverse, balanced and well-structured data. Spelling out how you will use the AI model as well as any limitations on the use of the model, will help reduce hallucinations. Testing your AI model rigorously before use is vital to preventing hallucinations, as is evaluating the model on an ongoing basis. Employing AI models to hallucinate and generate virtual environments can help game developers and VR designers imagine new worlds that take the user experience to the next level.", "published_at": null, "source_score": 0.985}, {"url": "https://neptune.ai/blog/llm-hallucinations", "title": "LLM Hallucinations 101: Why Do They Appear? Can We Avoid Them?", "content": "In short, hallucinations occur when a user instruction (prompt) leads the LLM to predict tokens that are not aligned with the expected answer or ground truth. These hallucinations mainly happen either because the correct token was not available or because the LLM failed to retrieve it. Models like Vectara’s HHEM-2.1-Open are encoder-decoder trained to detect hallucinations given a ground truth and an LLM response. Hallucinations can be prevented at different steps of the process an LLM uses to generate an output, and we can use this as the foundation for our categorization. * **Prompt engineering techniques**: Single-step prompt engineering strategies condition the model to how the response is generated, steering the LLM to think in a specific way that turns into better responses less prone to hallucinations.", "published_at": null, "source_score": 0.98389}, {"url": "https://openai.com/index/why-language-models-hallucinate/", "title": "Why language models hallucinate - OpenAI", "content": "[Skip to main content](https://openai.com/index/why-language-models-hallucinate/#main) [](https://openai.com/) *   [Research](https://openai.com/research/index/)  *   [For Developers](https://openai.com/api/)  *   [Company](https://openai.com/about/)  *   [Open Models](https://openai.com/open-models/) *   [About Us](https://openai.com/about/) *   [What are hallucinations?](https://openai.com/index/why-language-models-hallucinate/#what-are-hallucinations) *   [Teaching to the test](https://openai.com/index/why-language-models-hallucinate/#teaching-to-the-test) *   [A better way to grade evaluations](https://openai.com/index/why-language-models-hallucinate/#a-better-way-to-grade-evaluations) *   [How hallucinations originate from next-word prediction](https://openai.com/index/why-language-models-hallucinate/#how-hallucinations-originate-from-next-word-prediction) *   [Conclusions](https://openai.com/index/why-language-models-hallucinate/#conclusions) Our [new research paper⁠(opens in a new window)](https://arxiv.org/abs/2509.04664) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty. GPT‑5 has significantly fewer hallucinations [especially when reasoning⁠](https://openai.com/index/introducing-gpt-5/#:~:text=Building%20a%20more%20robust%2C%20reliable%2C%20and%20helpful%20model), but they still occur. Our [Model Spec⁠(opens in a new window)](https://model-spec.openai.com/2025-02-12.html#express_uncertainty) states that it is better to indicate uncertainty or ask for clarification than provide confident information that may be incorrect. *   [API log in(opens in a new window)](https://platform.openai.com/login) *   [About Us](https://openai.com/about/)", "published_at": null, "source_score": 0.98277}, {"url": "https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models", "title": "LLM Hallucinations in 2025: How to Understand and Tackle AI's ...", "content": "An AI hallucination occurs when a large language model (LLM) produces output that looks plausible but is factually wrong or unsupported by", "published_at": null, "source_score": 0.98077}, {"url": "https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)", "title": "Hallucination (artificial intelligence) - Wikipedia", "content": "[Jump to content](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#bodyContent) *   [Random article](https://en.wikipedia.org/wiki/Special:Random \"Visit a randomly selected article [x]\") *   [(Top)](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#) *   [1 Term](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Term)Toggle Term subsection *   [1.1 Origin](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Origin) *   [1.2 Definitions and alternatives](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Definitions_and_alternatives) *   [1.3 Criticism](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Criticism) *   [2.1 Causes](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Causes) *   [2.1.1 Hallucination from data](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Hallucination_from_data) *   [2.1.3 Interpretability research](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Interpretability_research) *   [2.2 Examples](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Examples) *   [3 In other modalities](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#In_other_modalities)Toggle In other modalities subsection *   [3.1 Object detection](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Object_detection) *   [3.3 Text-to-image generative AI](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Text-to-image_generative_AI) *   [4.1 Problems](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Problems) *   [4.2 Benefits](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Benefits) *   [5 Mitigation methods](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Mitigation_methods) *   [6 See also](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#See_also) *   [7 Notes](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#Notes) *   [8 References](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#References) *   [Article](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence) \"View the content page [c]\") *   [Talk](https://en.wikipedia.org/wiki/Talk:Hallucination_(artificial_intelligence) \"Discuss improvements to the content page [t]\") *   [Read](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) *   [Read](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) *   [What links here](https://en.wikipedia.org/wiki/Special:WhatLinksHere/Hallucination_(artificial_intelligence) \"List of all English Wikipedia pages containing links to this page [j]\") 28 languages[Add topic](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)#)", "published_at": null, "source_score": 0.97568}, {"url": "https://arxiv.org/abs/2509.04664", "title": "[2509.04664] Why Language Models Hallucinate - arXiv", "content": "Image 2: arxiv logo>cs> arXiv:2509.04664  **arXiv:2509.04664** (cs)  View a PDF of the paper titled Why Language Models Hallucinate, by Adam Tauman Kalai and Ofir Nachum and Santosh S. (or arXiv:2509.04664v1 [cs.CL] for this version) View a PDF of the paper titled Why Language Models Hallucinate, by Adam Tauman Kalai and Ofir Nachum and Santosh S. - [x] Bibliographic Explorer Toggle  - [x] Connected Papers Toggle  - [x] Litmaps Toggle  - [x] scite.ai Toggle  - [x] alphaXiv Toggle  - [x] Links to Code Toggle  - [x] DagsHub Toggle  - [x] GotitPub Toggle  - [x] Huggingface Toggle  - [x] Links to Code Toggle  - [x] ScienceCast Toggle  - [x] Replicate Toggle  - [x] Spaces Toggle  - [x] Spaces Toggle  - [x] Core recommender toggle ", "published_at": null, "source_score": 0.9681}, {"url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11815294/", "title": "The Clinicians' Guide to Large Language Models: A General ... - NIH", "content": "Another major cause of hallucinations stems from the very way certain LLMs are programmed. Indeed, most LLMs are auto-regressive; the term “auto", "published_at": null, "source_score": 0.96728}, {"url": "https://www.k2view.com/blog/llm-hallucination/", "title": "LLM hallucination risks and prevention - K2view", "content": "What causes an LLM hallucination? · Data deficiencies. Training data may contain biases, factual errors, or incomplete information. · Statistical blind spots.", "published_at": null, "source_score": 0.96518}, {"url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models/", "title": "A Gentle Introduction to Hallucinations in Large Language Models", "content": "Hallucinations in LLMs are when the model generates incorrect, nonsensical, or not real text, as if it is accurate.", "published_at": null, "source_score": 0.9636}, {"url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1622292/full", "title": "Survey and analysis of hallucinations in large language models", "content": "Hallucination in the context of Large Language Models (LLMs) refers to the generation of content that might not related to the input prompt or confirmed knowledge sources, even though the output may appear linguistically coherent (Ji et al., 2023; Maynez et al., 2020). • **Model-intrinsic hallucinations:** even when well organized prompts are used, LLMs may hallucinate due to limitations in training data, architectural biases, or inference-time sampling strategies (Bang and Madotto, 2023; OpenAI, 2023a; Chen et al., 2023). • *Model-dominant models* (e.g., DeepSeek 67B) show low PS but high MV, meaning hallucinations persist regardless of prompt variation, indicating internal knowledge limitations or inference biases. Citation: Anh-Hoang D, Tran V and Nguyen L-M (2025) Survey and analysis of hallucinations in large language models: attribution to prompting strategies or model behavior.", "published_at": null, "source_score": 0.9608}, {"url": "https://masterofcode.com/blog/hallucinations-in-llms-what-you-need-to-know-before-integration", "title": "LLM Hallucinations: What You Need to Know Before Integration", "content": "These concerns highlight the urgency and importance of addressing hallucination in LLMs to ensure the responsible Generative AI and LLM technologies use. By recognizing and understanding these types of hallucinations, we can better address the challenges and limitations of LLMs. Effective mitigation strategies can help improve the accuracy, coherence, and trustworthiness of LLM-generated responses across various domains. To address the challenges posed by hallucination in LLMs, organizations integrating Generative AI, such as ChatGPT, can employ various strategies to minimize the occurrence of inaccurate or misleading responses. LLM hallucination poses significant challenges in generating accurate and reliable responses, stemming from factors such as source-reference divergence, biased training data, and privacy concerns, leading to potential spread of misinformation, discriminatory content, and privacy violations.", "published_at": null, "source_score": 0.95547}, {"url": "https://www.nature.com/articles/s41586-024-07421-0", "title": "Detecting hallucinations in large language models using semantic ...", "content": "Large language model (LLM) systems, such as ChatGPT[1](https://www.nature.com/articles/s41586-024-07421-0#ref-CR1 \"GPT-4 technical report. (2023).\"), can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers[3](https://www.nature.com/articles/s41586-024-07421-0#ref-CR3 \"Xiao, Y. 16th Conference of the European Chapter of the Association for Computational Linguistics 2734–2744 (Association for Computational Linguistics, 2021).\"),[4](https://www.nature.com/articles/s41586-024-07421-0#ref-CR4 \"Rohrbach, A., Hendricks, L. Surv.55, 248 (2023).\") for natural language generation systems using large language models (LLMs), such as ChatGPT[1](https://www.nature.com/articles/s41586-024-07421-0#ref-CR1 \"GPT-4 technical report. Surv.55, 248 (2023).\"),[10](https://www.nature.com/articles/s41586-024-07421-0#ref-CR10 \"Maynez, J., Narayan, S., Bohnet, B. & Tetreault, J.) 1906–1919 (Association for Computational Linguistics, 2020).\"),[11](https://www.nature.com/articles/s41586-024-07421-0#ref-CR11 \"Filippova, K. We focus on a subset of hallucinations which we call ‘confabulations’[12](https://www.nature.com/articles/s41586-024-07421-0#ref-CR12 \"Berrios, G. Our method makes progress on a portion of the problem of providing scalable oversight[15](https://www.nature.com/articles/s41586-024-07421-0#ref-CR15 \"Amodei, D.", "published_at": null, "source_score": 0.95514}, {"url": "https://arxiv.org/abs/2311.05232", "title": "[2311.05232] A Survey on Hallucination in Large Language Models", "content": "[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025) [Skip to main content](https://arxiv.org/abs/2311.05232#content) [](https://arxiv.org/IgnoreMe) [Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced) *   [Login](https://arxiv.org/login) *   [Help Pages](https://info.arxiv.org/help) *   [About](https://info.arxiv.org/about) Cite as:[arXiv:2311.05232](https://arxiv.org/abs/2311.05232) [cs.CL] (or [arXiv:2311.05232v2](https://arxiv.org/abs/2311.05232v2) [cs.CL] for this version) **[[v1]](https://arxiv.org/abs/2311.05232v1)** Thu, 9 Nov 2023 09:25:37 UTC (983 KB) [](https://arxiv.org/abs/2311.05232)Full-text links: *   [View PDF](https://arxiv.org/pdf/2311.05232) *   [HTML (experimental)](https://arxiv.org/html/2311.05232v2) *   [TeX Source](https://arxiv.org/src/2311.05232) [cs](https://arxiv.org/abs/2311.05232?context=cs) ### [3 blog links](https://arxiv.org/tb/2311.05232) ([what is this?](https://info.arxiv.org/help/trackback.html))  Data provided by: [](https://arxiv.org/abs/2311.05232) [![Image 7: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2311.05232&description=A%20Survey%20on%20Hallucination%20in%20Large%20Language%20Models:%20Principles,%20Taxonomy,%20Challenges,%20and%20Open%20Questions \"Bookmark on BibSonomy\")[![Image 8: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2311.05232&title=A%20Survey%20on%20Hallucination%20in%20Large%20Language%20Models:%20Principles,%20Taxonomy,%20Challenges,%20and%20Open%20Questions \"Bookmark on Reddit\") Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_ *   [Author](https://arxiv.org/abs/2311.05232) *   [Venue](https://arxiv.org/abs/2311.05232) *   [Institution](https://arxiv.org/abs/2311.05232) *   [Topic](https://arxiv.org/abs/2311.05232) [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html). [Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.05232) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))  *   [About](https://info.arxiv.org/about) *   [Help](https://info.arxiv.org/help) *   [Contact](https://info.arxiv.org/help/contact.html) *   [Subscribe](https://info.arxiv.org/help/subscribe) *   [Copyright](https://info.arxiv.org/help/license/index.html) *   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html) *   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)", "published_at": null, "source_score": 0.95211}, {"url": "https://ieeexplore.ieee.org/document/10542617", "title": "Hallucinations in Large Language Models (LLMs) - IEEE Xplore", "content": "Hallucinations in Large Language Models (LLMs) | IEEE Conference Publication | IEEE Xplore The recent advancements in neural network architectures, particularly transformers, have played a crucial role in the rapid progress of Large Language Models (LLMs). The recent advancements in neural network architectures, particularly transformers, have played a crucial role in the rapid progress of Large Language Models (LLMs). These models have enabled machines to generate new data (human-like), driving significant developments in Natural Language Processing (NLP). Large Language Models (LLMs) are a kind of AI that are trained on huge datasets [9], [10]. LLMs are advanced NLP (Natural Language Processing) models that use a deep neural network architecture designed to understand, generate, and process human language at a sophisticated level [11].", "published_at": null, "source_score": 0.94973}, {"url": "https://www.baeldung.com/cs/llm-fix-hallucinations-why-happen", "title": "Hallucinations in Large Language Models: Causes, Challenges ...", "content": "Causes of Hallucinations in Large Language Models # Hallucinations in Large Language Models: Causes, Challenges, and Mitigation We’ll cover aspects like the training data and the probabilistic nature of large language models. **Large language models can generate responses that seem logical or coherent but contain incorrect or inconsistent information. Causes of Hallucinations in Large Language Models Some methods can help mitigate hallucinations in large language models. **By letting models check their responses against outside sources of quality information, we can significantly reduce hallucinations.** In this article, we learned that hallucinations are a significant challenge in large language models. The inherent limits of the training data and model architecture cause this problem. To tackle these challenges, we need better training data, model architectures that handle ambiguities, and real-time fact-checking.", "published_at": null, "source_score": 0.94561}, {"url": "https://medium.com/@tam.tamanna18/understanding-llm-hallucinations-causes-detection-prevention-and-ethical-concerns-914bc89128d0", "title": "Understanding LLM Hallucinations. Causes, Detection, Prevention ...", "content": "LLM hallucinations occur when a model generates text that is factually incorrect, inconsistent, or entirely made up.", "published_at": null, "source_score": 0.94552}]}